{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# !pip install -q git+https://github.com/tensorflow/docs\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize   \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_paths(path):\n",
    "    # Get directory and filename\n",
    "    dirname = os.path.dirname(path)\n",
    "    basename = os.path.basename(path)\n",
    "    \n",
    "    # Create path for each dataset\n",
    "    train_path = os.path.join(dirname, 'train_' + basename)\n",
    "    test_path = os.path.join(dirname, 'test_' + basename)\n",
    "    val_path = os.path.join(dirname, 'val_' + basename)\n",
    "    \n",
    "    return train_path, test_path, val_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 1000\n",
    "TEST_SIZE = 0.15 \n",
    "VAL_SIZE= 0.15\n",
    "\n",
    "trump_path = './input/hashtag_donaldtrump.csv'\n",
    "biden_path = './input/hashtag_joebiden.csv'\n",
    "\n",
    "train_trump, test_trump, val_trump = train_test_val_paths(trump_path)\n",
    "train_biden, test_biden, val_biden = train_test_val_paths(biden_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN - TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_split(path):\n",
    "    \n",
    "    train_path, test_path, val_path = train_test_val_paths(path)\n",
    "    \n",
    "    # Create an empty csv file to append chunks\n",
    "    header_df = pd.DataFrame(columns=['tweet'])\n",
    "    header_df.to_csv(train_path, index=False)\n",
    "    header_df.to_csv(test_path, index=False)\n",
    "    header_df.to_csv(val_path, index=False)\n",
    "    \n",
    "    # Define the probabilities to select each path\n",
    "    TRAIN_SIZE = 1 - TEST_SIZE - VAL_SIZE\n",
    "    probabilities = [TRAIN_SIZE, TEST_SIZE, VAL_SIZE]\n",
    "    paths = [train_path, test_path, val_path]\n",
    "    \n",
    "    # Create the DataFrame Reader\n",
    "    df = pd.read_csv(path, \n",
    "                     lineterminator='\\n', \n",
    "                     chunksize=BATCH_SIZE, \n",
    "                     usecols=['tweet'])\n",
    "\n",
    "    # Split the dataset\n",
    "    for chunk in df:\n",
    "        path = np.random.choice(paths, p=probabilities)\n",
    "        chunk.to_csv(path, index=False, mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_split(path=trump_path)\n",
    "get_tweets_split(path=biden_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(text):\n",
    "    # Lower case\n",
    "    lower_text = tf.strings.lower(text)\n",
    "\n",
    "    # Remove new lines\n",
    "    lower_text = tf.strings.regex_replace(input=lower_text, \n",
    "                                          pattern='\\n', \n",
    "                                          rewrite=' ')\n",
    "\n",
    "    # Remove URLs\n",
    "    free_url_text = tf.strings.regex_replace(input=lower_text, \n",
    "                                             pattern=\"http\\S+\", \n",
    "                                             rewrite=' ')\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = emoji.get_emoji_regexp().pattern\n",
    "    emoji_pattern = emoji_pattern.replace('#','') # There is a # emoji\n",
    "    free_emoji_text = tf.strings.regex_replace(input=free_url_text, \n",
    "                                               pattern='[%s]' % re.escape(emoji_pattern),\n",
    "                                               rewrite=' ')\n",
    "\n",
    "    # Remove punctuation\n",
    "    # punctuation_pattern = string.punctuation.replace('#', '').replace('@','').replace(\"\\'\", '')\n",
    "    punctuation_pattern = string.punctuation\n",
    "    free_punctuation_text =  tf.strings.regex_replace(free_emoji_text,\n",
    "                                                      '[%s]' % re.escape(punctuation_pattern),\n",
    "                                                      ' ')\n",
    "    return free_punctuation_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Original function in: https://www.kaggle.com/gatandubuc/donald-trump-vs-joe-biden\n",
    "#\n",
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Dictionary with languages and unique stopwords seen in analyzed text\n",
    "    @rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n",
    "    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n",
    "    '''\n",
    "    \n",
    "    tokens = wordpunct_tokenize(text.numpy().decode())\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # Compute per language included in nltk number of unique stopwords appearing in analyzed text\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "    \n",
    "    most_rated_language = max(languages_ratios, key=languages_ratios.get)\n",
    "    \n",
    "    if most_rated_language != 'english':\n",
    "        text = 'EMPTY'\n",
    "    \n",
    "    return text    \n",
    "    # return most_rated_language == 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_is_english(text):\n",
    "    py_function = tf.py_function(is_english, [text], text.dtype)\n",
    "    py_function.set_shape(text.shape)\n",
    "    return py_function\n",
    "\n",
    "def filter_by_EMPTY(text):\n",
    "    return not tf.math.equal(text, 'EMPTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path):\n",
    "    \n",
    "    # Create the dataset\n",
    "    ds = tf.data.experimental.make_csv_dataset(path, \n",
    "                                              batch_size=BATCH_SIZE, \n",
    "                                              select_columns=['tweet'],\n",
    "                                              num_epochs=1)\n",
    "    # Get the data in the tweet column\n",
    "    ds = ds.map(lambda x: x['tweet'])\n",
    "    \n",
    "    # Standarize the input (lower case, remove URL, remove emoticons...)\n",
    "    ds = ds.map(custom_standardization)\n",
    "    \n",
    "    # Filter by language\n",
    "    # Step 1: change non english text by the word EMPTY\n",
    "    # This step must be unbatched because is_english \n",
    "    # doesn't work with batches, only single rows (text)\n",
    "    ds = ds.unbatch().map(tf_is_english)\n",
    "    \n",
    "    # Step 2: remove all ocurrences of the word EMPTY\n",
    "    ds = ds.filter(lambda x: not tf.math.equal(x, 'EMPTY'))\n",
    "    \n",
    "    # Batch the output\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_class_column(dataset, class_value):\n",
    "    return dataset.map(lambda x: (x, tf.repeat(class_value, tf.size(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(trump_path, biden_path, class_atributte=True):\n",
    "    \n",
    "    trump_ds = create_dataset(trump_path)\n",
    "    biden_ds = create_dataset(biden_path)\n",
    "    \n",
    "    # Add class atributte\n",
    "    if class_atributte:\n",
    "        trump_ds = add_class_column(trump_ds, 0)\n",
    "        biden_ds = add_class_column(biden_ds, 1)\n",
    "    \n",
    "    datasets = [\n",
    "        trump_ds.unbatch(),\n",
    "        biden_ds.unbatch()\n",
    "    ]\n",
    "    \n",
    "    # Equally merge datasets\n",
    "    trump_biden_ds = tf.data.experimental.sample_from_datasets(datasets=datasets, weights=[0.5, 0.5], seed=42)\n",
    "        \n",
    "    # Batch the dataset\n",
    "    trump_biden_ds = trump_biden_ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    return trump_biden_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = input_pipeline(train_trump, train_biden, class_atributte=True)\n",
    "val_ds = input_pipeline(val_trump, val_biden, class_atributte=True)\n",
    "test_ds = input_pipeline(test_trump, test_biden, class_atributte=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'i bet if you could get  joebiden to open up around a beer or two he would say he is owed the right to benefit his family thru  govt because of all his  public service  he gave up his life to help us so he can help them   swampthing'\n",
      " b' joebiden calls him a lier and then insults him for being overweight   biden shows his low iq daily   maybe joe should hold town halls in kindergarten   he\\xe2\\x80\\x99ll be amongst equals and no one will say anything when he takes his afternoon nap  '], shape=(2,), dtype=string)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_ds))\n",
    "print(features[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(map(len,features.numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000\n",
    "SEQUENCE_LENGTH = 260\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=lambda x: x,\n",
    "    max_tokens=MAX_FEATURES,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(vectorize_text)\n",
    "val_ds = val_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure datasets for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(MAX_FEATURES + 1, EMBEDDING_DIM),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    return [\n",
    "#         tfdocs.modeling.EpochDots(report_every=100),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3)\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.6937 - binary_accuracy: 0.5061 - val_loss: 0.6939 - val_binary_accuracy: 0.4956\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.6922 - binary_accuracy: 0.5102 - val_loss: 0.6937 - val_binary_accuracy: 0.4956\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.6925 - binary_accuracy: 0.5102 - val_loss: 0.6934 - val_binary_accuracy: 0.4956\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.6917 - binary_accuracy: 0.5164 - val_loss: 0.6932 - val_binary_accuracy: 0.4956\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.6914 - binary_accuracy: 0.5205 - val_loss: 0.6931 - val_binary_accuracy: 0.4956\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\raulc\\anaconda3\\envs\\tfdeeplearning_v3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\raulc\\anaconda3\\envs\\tfdeeplearning_v3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./saved_model/my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./saved_model/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6931 - binary_accuracy: 0.4956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6930584907531738, 0.49561402201652527]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-a6a389ce8844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplotter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHistoryPlotter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'binary_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmoothing_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplotter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tfdeeplearning_v3\\lib\\site-packages\\tensorflow_docs\\plots\\__init__.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, histories, metric, smoothing_std)\u001b[0m\n\u001b[0;32m     78\u001b[0m       \u001b[0msmoothing_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmoothing_std\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m       \u001b[1;31m# Remember name->color asociations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_table\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'items'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_accuracy', smoothing_std=10)\n",
    "plotter.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfdeeplearning_v3]",
   "language": "python",
   "name": "conda-env-tfdeeplearning_v3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
